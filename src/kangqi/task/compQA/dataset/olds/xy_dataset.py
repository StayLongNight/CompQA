import os
import json
import codecs
import cPickle
import numpy as np

from kangqi.util.LogUtil import LogInfo

from .u import load_simpq, load_webq, load_compq
from ..u import is_mediator_as_expect, get_domain, get_range, load_super_type_dict

from .schema import build_sc_from_line, Schema
from .kq_schema import CompqSchema


""" Schema semantic quality control: code from debugging/general_two_hop_check.py """

schema_level_dict = {'strict': 0, 'elegant': 1, 'coherent': 2, 'general': 3}

STRICT = 0          # 1-hop or 2-hop with mediators found in "mediator.tsv"
ELEGANT = 1         # allowing 2-hop where pred1.range == pred2.domain
COHERENT = 2        # allowing 2-hop where pred1.range \in pred2.domain
GENERAL = 3         # allowing all the 2-hop schemas


def schema_classification(sc, super_type_dict):
    for category, focus, pred_seq in sc.raw_paths:
        if category != 'Main':
            continue  # only consider main path
        elif len(pred_seq) == 1:
            return 0
        elif is_mediator_as_expect(pred=pred_seq[0]):
            return 0
        else:
            p1_range = get_range(pred_seq[0])
            p2_domain = get_domain(pred_seq[1])
            if p1_range == p2_domain:
                return 1
            elif p1_range in super_type_dict and p2_domain in super_type_dict[p1_range]:
                return 2
            else:
                return 3
    return 3


# Load data from the schemas generated by xianyang
def load_schema_by_xy_protocol(schema_fp, fb_helper, sc_len_dist, path_len_dist, sc_max_len, path_max_len):
    candidate_list = []             # store candidates (dedup, and no matter usable or not)
    path_list_str_set = set([])     # store all schemas/skeletons in this data, just for dedup
    with codecs.open(schema_fp, 'r', 'utf-8') as br:
        schema_data = json.load(br)
        for data_item in schema_data:
            sk_p, sk_r, sk_f1 = [float(data_item[x]) for x in ['P', 'R', 'F1']]
            sk_line = data_item['basicgraph']
            sc, path_list_str = build_sc_from_line(sk_line, fb_helper)
            if path_list_str not in path_list_str_set:
                path_list_str_set.add(path_list_str)
                sc_len_dist.append(len(sc.path_list))
                for path in sc.path_list:
                    path_len_dist.append(len(path))
                if sc.is_schema_ok(sc_max_len=sc_max_len, path_max_len=path_max_len):
                    sc.p = sk_p
                    sc.r = sk_r
                    sc.f1 = sk_f1
                    candidate_list.append(sc)
            local_schema_list = data_item['schemas']
            for local_schema in local_schema_list:
                if 'P' in local_schema:
                    sc_p, sc_r, sc_f1 = [float(local_schema[x]) for x in ['P', 'R', 'F1']]
                else:
                    sc_p, sc_r, sc_f1 = sk_p, sk_r, sk_f1  # derived from the skeleton
                sc_line = local_schema['schema']
                sc, path_list_str = build_sc_from_line(sc_line, fb_helper)
                if path_list_str not in path_list_str_set:
                    path_list_str_set.add(path_list_str)
                    sc_len_dist.append(len(sc.path_list))
                    for path in sc.path_list:
                        path_len_dist.append(len(path)-1)
                    if sc.is_schema_ok(sc_max_len=sc_max_len, path_max_len=path_max_len):
                        sc.p = sc_p
                        sc.r = sc_r
                        sc.f1 = sc_f1
                        candidate_list.append(sc)
    return candidate_list, path_list_str_set


def load_schema_by_kq_protocol(schema_fp, sc_len_dist, path_len_dist, sc_max_len, path_max_len):
    """
    Read the schema files generated by KQ.
    Used before 12/05/2017
    """
    candidate_list = []
    path_list_str_set = set([])
    with codecs.open(schema_fp, 'r', 'utf-8') as br:
        # sc_sz = int(br.readline().strip())
        # for sc_idx in range(sc_sz):
        #     line = br.readline().strip()
        for sc_idx, line in enumerate(br.readlines()):
            sc_dict = json.loads(line.strip())
            sc_dict['original_idx'] = sc_idx
            sc = Schema(**sc_dict)
            path_list_str = sc.disp()
            if path_list_str not in path_list_str_set:
                path_list_str_set.add(path_list_str)
                sc_len_dist.append(len(sc.path_list))
                for path in sc.path_list:
                    path_len_dist.append(len(path))
                if sc.is_schema_ok(sc_max_len=sc_max_len, path_max_len=path_max_len):
                    candidate_list.append(sc)
    return candidate_list, path_list_str_set


def load_schema_by_kqnew_protocol(schema_fp, gather_linkings,
                                  sc_len_dist, path_len_dist, sc_max_len, schema_level):
    """
    Read the schema files generated by KQ.
    Using the schema in kq_schema.py
    We read raw paths from json files, and convert them into path_list on-the-fly.
    Used after 12/05/2017.
    schema level: 0/1/2/3 (STRICT/ELEGANT/COHERENT/GENERAL)
    """
    LogInfo.logs('Schema level: %s', schema_level)
    schema_level = schema_level_dict[schema_level]
    super_type_dict = load_super_type_dict()
    candidate_list = []
    path_list_str_set = set([])
    with codecs.open(schema_fp, 'r', 'utf-8') as br:
        lines = br.readlines()
        for ori_idx, line in enumerate(lines):
            sc = CompqSchema.read_schema_from_json(json_line=line, gather_linkings=gather_linkings)
            sc.ori_idx = ori_idx + 1
            sc.construct_path_list()        # create the path_list on-the-fly
            path_list_str = sc.disp()
            """
                from the perspective of candidate searching in eff_candgen,
                since we treat main path and constraint path in different direction,
                there's no so-called duplicate schema at all.
                171226: Except for duplicate entities in EL results.
            """
            path_list_str_set.add(path_list_str)
            sc_len_dist.append(len(sc.path_list))
            for path in sc.path_list:
                path_len_dist.append(len(path))
            if len(sc.path_list) <= sc_max_len and schema_classification(sc, super_type_dict) <= schema_level:
                candidate_list.append(sc)
    return candidate_list, path_list_str_set, len(lines)


class QScDataset(object):

    def __init__(self, data_name, data_dir, file_list_name, protocol_name,
                 q_max_len, sc_max_len, path_max_len, item_max_len, schema_level,
                 wd_emb_util, kb_emb_util, fb_helper, verbose=0):
        self.w_size = self.e_size = self.p_size = self.array_num = 0

        self.q_list = None
        self.q_cand_dict = None
        self.q_link_dict = None
        self.q_words_dict = None
        self.np_data_list = []      # several numpy arrays for all schemas

        self.w_dict = None
        self.e_dict = None
        self.p_dict = None      # the actual <item, index> dictionary

        self.w_init_emb = None
        self.e_init_emb = None
        self.p_init_emb = None  # the actual initial embedding data

        self.wd_emb_util = wd_emb_util
        self.kb_emb_util = kb_emb_util
        self.fb_helper = fb_helper

        self.data_name = data_name
        self.data_dir = data_dir
        self.file_list_name = file_list_name
        self.protocol_name = protocol_name

        self.q_max_len = q_max_len
        self.sc_max_len = sc_max_len
        self.path_max_len = path_max_len
        self.item_max_len = item_max_len
        self.schema_level = schema_level
        self.verbose = verbose

        self.save_dir = '%s/%s_useful-%d-%d-%d-%d-%s' % (
            data_dir, file_list_name, q_max_len, sc_max_len, path_max_len, item_max_len, schema_level)
        self.size_fp = self.save_dir + '/wep_size'
        # saving the size of used embedding matrix, since not all data are used
        self.dict_fp = self.save_dir + '/wep_dict.cPickle'
        self.dump_fp = self.save_dir + '/cand.cPickle'
        self.init_mat_fp = self.save_dir + '/init_emb.pydump'
        # saving the initial embedding of W/E/P for the current data

    def prepare_all_data(self):         # WEP size / item dict / init emb / schema candidate
        if self.data_name == 'WebQ':
            qa_list = load_webq()
        elif self.data_name == 'CompQ':
            qa_list = load_compq()
        else:
            qa_list = load_simpq()
        self.q_list = [qa['utterance'] for qa in qa_list]

        verbose = self.verbose
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)
        LogInfo.begin_track('Loading schema dataset from [%s] ...', self.data_dir)
        self.wd_emb_util.load_words()
        sc_len_dist = []  # count the distribution of number of paths in a schema
        path_len_dist = []
        q_len_dist = []

        total_cand_size = useful_cand_size = 0
        list_fp = '%s/%s' % (self.data_dir, self.file_list_name)
        with open(list_fp, 'r') as br:
            schema_fp_list = map(lambda line: '%s/%s' % (self.data_dir, line.strip()),
                                 br.readlines())
        LogInfo.logs('%d schema files found in [%s].', len(schema_fp_list), self.file_list_name)

        self.q_cand_dict = {}       # <q_idx, candidates>
        self.q_link_dict = {}       # <q_idx, gather_linkings>
        for scan_idx, schema_fp in enumerate(schema_fp_list):
            link_fp = schema_fp[0: schema_fp.rfind('_')] + '_links'

            if scan_idx % 100 == 0:
                LogInfo.logs('%d / %d scanned.', scan_idx, len(schema_fp_list))
            q_idx = int(schema_fp.split('/')[-1].split('_')[0])  # data/compQA/xy_q_schema/train/0-99/1_2 --> 1
            q = self.q_list[q_idx]
            q_len_dist.append(len(q.split(' ')))        # just a rough count of question length
            if os.path.isfile(link_fp):     # for SimpleQuestions
                with open(link_fp, 'rb') as br:
                    gather_linkings = cPickle.load(br)
                self.q_link_dict[q_idx] = gather_linkings

            if verbose >= 1:
                LogInfo.begin_track('scan_idx = %d, q_idx = %d:', scan_idx, q_idx)
                LogInfo.logs('Q: %s', q.encode('utf-8'))

            if self.protocol_name == 'XY':
                candidate_list, path_list_str_set = load_schema_by_xy_protocol(
                    schema_fp=schema_fp, fb_helper=self.fb_helper,
                    sc_len_dist=sc_len_dist, path_len_dist=path_len_dist,
                    sc_max_len=self.sc_max_len, path_max_len=self.path_max_len)
                total_cand_size += len(path_list_str_set)
            elif self.protocol_name == 'KQ':
                candidate_list, path_list_str_set = load_schema_by_kq_protocol(
                    schema_fp=schema_fp,
                    sc_len_dist=sc_len_dist, path_len_dist=path_len_dist,
                    sc_max_len=self.sc_max_len, path_max_len=self.path_max_len)
                total_cand_size += len(path_list_str_set)
            else:       # KQNEW
                candidate_list, _, total_lines = load_schema_by_kqnew_protocol(
                    schema_fp=schema_fp, gather_linkings=self.q_link_dict[q_idx],
                    sc_len_dist=sc_len_dist, path_len_dist=path_len_dist,
                    sc_max_len=self.sc_max_len, schema_level=self.schema_level)
                total_cand_size += total_lines
            useful_cand_size += len(candidate_list)

            if verbose >= 1:
                # local_useful_size = len(candidate_list)
                # local_size = len(path_list_str_set)
                srt_cand_list = sorted(candidate_list, key=lambda _sc: _sc.f1, reverse=True)
                for rank_idx, sc in enumerate(srt_cand_list[:5]):
                    LogInfo.logs('#%d: F1 = %.6f, schema = [%s]', rank_idx + 1, sc.f1, sc.disp())
                # LogInfo.logs('Local useful candidates = %d / %d (%.3f%%)',
                #              local_useful_size, local_size,
                #              100. * local_useful_size / local_size if local_size > 0 else 0.)
                LogInfo.logs('Candidate size = %d', len(candidate_list))
                LogInfo.end_track()
            self.q_cand_dict[q_idx] = candidate_list

        q_size = len(self.q_cand_dict)
        LogInfo.begin_track('[STAT] %d questions scanned:', q_size)
        LogInfo.logs('Total schemas = %d', total_cand_size)
        LogInfo.logs('Useful schemas = %d (%.3f%%)',
                     useful_cand_size, 100. * useful_cand_size / total_cand_size)
        LogInfo.logs('Avg candidates = %.3f', 1. * useful_cand_size / q_size)
        q_len_dist = np.array(q_len_dist)
        sc_len_dist = np.array(sc_len_dist)
        path_len_dist = np.array(path_len_dist)
        cand_size_dist = np.array([len(v) for v in self.q_cand_dict.values()])
        for show_name, show_dist in zip(['q_len', 'cand_size', 'sc_len', 'path_len'],
                                        [q_len_dist, cand_size_dist, sc_len_dist, path_len_dist]):
            LogInfo.begin_track('Show %s distribution:', show_name)
            for pos in (25, 50, 75, 90, 95, 99, 99.9, 100):
                LogInfo.logs('Percentile = %.1f%%: %.6f', pos, np.percentile(show_dist, pos))
            LogInfo.end_track()
        LogInfo.end_track()  # end of showing stat.
        # Displaying candidate size (DESC) and save into file
        cand_disp_list = []
        for q_idx in self.q_cand_dict.keys():
            q = self.q_list[q_idx]
            cand_sz = len(self.q_cand_dict[q_idx])
            cand_disp_list.append((q_idx, q, cand_sz))
        cand_disp_list.sort(key=lambda tup: tup[2], reverse=True)
        cand_disp_fp = '%s/%s_cand_size' % (self.data_dir, self.file_list_name)
        with codecs.open(cand_disp_fp, 'w', 'utf-8') as bw:
            for q_idx, q, cand_sz in cand_disp_list:
                bw.write('%d\t#%d\t%s\n' % (cand_sz, q_idx, q))

        # Get ready for constructing schema details (numpy list)
        if self.data_name == 'SimpQ':
            data_series = self.build_schema_np_data_simple()
        else:
            data_series = self.build_schema_np_data_complex()

        (self.q_words_dict, self.np_data_list,
         self.w_dict, self.e_dict, self.p_dict,
         self.w_init_emb, self.e_init_emb, self.p_init_emb) = data_series
        self.w_size = self.w_init_emb.shape[0]
        self.e_size = self.e_init_emb.shape[0]
        self.p_size = self.p_init_emb.shape[0]

        self.save_size()
        self.save_cands()
        self.save_dicts()
        self.save_init_emb()

        LogInfo.end_track()  # end of build dataset

    # ================ Create detail numpy array and initial embedding matrix ================ #

    # def build_schema_np_data_original_complex(self):
    #     """
    #     Goal: given all the candidate schemas, build all the necessary numpy input
    #     ** Used in WebQ and SimpQ **
    #     ** Used before 12/05/2017 (Deprecated) **
    #     ** Target models: q_sc_dyn_eval_model, q_sc_hinge_model (Deprecated) **
    #     :return: A list of following information
    #     1. q_words_dict, the word-index tensor of each question
    #     2. np_data, all the numpy data as the input of the tensor
    #     3. w/e/p_dict, the actual <word/mid, index> dictionary (ignoring unused items)
    #     3. w/e/p_init_emb, the corresponding initial embedding parameter
    #     """
    #     sc_max_len = self.sc_max_len
    #     path_max_len = self.path_max_len
    #     item_max_len = self.item_max_len
    #
    #     cand_size = sum([len(v) for v in self.q_cand_dict.values()])
    #     LogInfo.begin_track('Building np data for %d candidates:', cand_size)
    #
    #     e_dict, p_dict = self.load_necessary_entity_predicate_dict()
    #     w_dict = {}     # <item, idx>: used for constructing the actual initial embedding matrix
    #
    #     q_words_dict = {}
    #     np_data = []
    #     for shape in [(cand_size,),                             # sc_len
    #                   (cand_size, sc_max_len),                  # focus_kb
    #                   (cand_size, sc_max_len, item_max_len),    # focus_item
    #                   (cand_size, sc_max_len),                  # focus_item_len
    #                   (cand_size, sc_max_len),                  # path_len
    #                   (cand_size, sc_max_len, path_max_len),    # path_kb
    #                   (cand_size, sc_max_len, path_max_len, item_max_len),  # path_item
    #                   (cand_size, sc_max_len, path_max_len)]:   # path_item_len
    #         LogInfo.logs('Adding tensor with shape %s ... ', shape)
    #         np_data.append(np.zeros(shape=shape, dtype='int32'))
    #     [sc_len, focus_kb, focus_item, focus_item_len,
    #      path_len, path_kb, path_item, path_item_len] = np_data
    #
    #     data_idx = 0
    #     q_list = sorted(self.q_cand_dict.keys())
    #     for scan_idx, q_idx in enumerate(q_list):
    #         if scan_idx % 500 == 0:
    #             LogInfo.logs('%d question scanned, filled with %d / %d schemas.',
    #                          scan_idx, data_idx, cand_size)
    #         q = self.q_list[q_idx]
    #         q_word_seq = self.wd_emb_util.surface_split_naive(q)
    #         q_words = self.get_word_idx_list_from_string(word_seq=q_word_seq, w_dict=w_dict)
    #         if len(q_words) > self.q_max_len:
    #             q_words = q_words[:self.q_max_len]
    #         q_words_dict[q_idx] = q_words
    #
    #         for sc_idx, sc in enumerate(self.q_cand_dict[q_idx]):
    #             assert isinstance(sc, Schema)
    #             sc.use_idx = data_idx
    #             sc_len[data_idx] = len(sc.path_list)
    #             sc.path_words_list = []     # ready to get the words of each path
    #             # if sc_idx <= 5:
    #             #     LogInfo.begin_track('Show detail ... ')
    #             for sk_idx, path in enumerate(sc.path_list):
    #                 local_words = []
    #                 focus = path[0]
    #                 focus_kb[data_idx, sk_idx] = e_dict[focus]    # the focus must be in the e_dict
    #                 focus_name = self.fb_helper.get_item_name(focus)
    #                 focus_word_seq = self.wd_emb_util.surface_split_naive(focus_name)
    #                 local_words.append(focus_name)
    #                 item_indices = self.get_word_idx_list_from_string(word_seq=focus_word_seq, w_dict=w_dict)
    #                 # item_indices = wd_emb_util.q2idx(focus_name)
    #                 item_len = min(len(item_indices), item_max_len)
    #                 focus_item_len[data_idx, sk_idx] = item_len
    #                 focus_item[data_idx, sk_idx, :item_len] = item_indices[:item_len]
    #                 # if sc_idx <= 5:
    #                 #     LogInfo.logs('%s --> %s, item_len = %d', focus, focus_name.encode('utf-8'), item_len)
    #
    #                 path_len[data_idx, sk_idx] = len(path) - 1
    #                 for pred_idx, pred in enumerate(path[1:]):
    #                     path_kb[data_idx, sk_idx, pred_idx] = p_dict[pred]      # the same as above
    #                     pred_name = self.fb_helper.get_item_name(pred)
    #                     pred_word_seq = self.wd_emb_util.surface_split_naive(pred_name)
    #                     local_words.append(pred_name)
    #                     item_indices = self.get_word_idx_list_from_string(word_seq=pred_word_seq, w_dict=w_dict)
    #                     # item_indices = wd_emb_util.q2idx(pred_name)
    #                     item_len = min(len(item_indices), item_max_len)
    #                     path_item_len[data_idx, sk_idx, pred_idx] = item_len
    #                     path_item[data_idx, sk_idx, pred_idx, :item_len] = item_indices[:item_len]
    #                     # if sc_idx <= 5:
    #                     #     LogInfo.logs('%s --> %s, item_len = %d', pred, pred_name.encode('utf-8'), item_len)
    #                 sc.path_words_list.append(local_words)
    #
    #             # if sc_idx <= 5:
    #             #     LogInfo.end_track()
    #             data_idx += 1
    #     assert data_idx == cand_size
    #     LogInfo.end_track('np_data build complete.')
    #
    #     LogInfo.begin_track('now creating actual initial embedding matrix ...')
    #     w_init_emb, e_init_emb, p_init_emb = self.create_wep_init_emb(w_dict, e_dict, p_dict)
    #     LogInfo.end_track()
    #     return q_words_dict, np_data, w_dict, e_dict, p_dict, w_init_emb, e_init_emb, p_init_emb

    def build_schema_np_data_simple(self):
        """
        Given candidate pools, return all the information about these candidates
        :return: q_words_dict: <q_idx, word_index_sequence>
                 np_data: A list of numpy array representing the input data to the model
                 w/e/p_dict: the <word/mid, index> dictionary (ignoring unused items)
                 w/e/p_init_emb: the corresponding initial embedding parameters
        """
        path_max_len = self.path_max_len
        pword_max_len = self.path_max_len * self.item_max_len
        # restrict the max length of path and path words

        # Step 1: Allocate memory for saving numpy data of all the candidates
        cand_size = sum([len(v) for v in self.q_cand_dict.values()])
        LogInfo.begin_track('Building np data for %d candidates:', cand_size)
        e_dict, p_dict = self.load_necessary_entity_predicate_dict()
        w_dict = {}  # <item, idx>: used for constructing the actual initial embedding matrix
        q_words_dict = {}
        np_data = []
        for shape in [(cand_size, path_max_len),    # preds
                      (cand_size, ),                # preds_len
                      (cand_size, pword_max_len),   # pwords
                      (cand_size, )]:               # pwords_len
            LogInfo.logs('Adding tensor with shape %s ... ', shape)
            np_data.append(np.zeros(shape=shape, dtype='int32'))
        [preds, preds_len, pwords, pwords_len] = np_data
        self.array_num = len(np_data)

        # Step 2: Scan each candidate, get ready to fill in with their numpy data
        data_idx = 0
        q_list = sorted(self.q_cand_dict.keys())
        for scan_idx, q_idx in enumerate(q_list):
            if scan_idx % 500 == 0:
                LogInfo.logs('%d question scanned, filled with %d / %d schemas.',
                             scan_idx, data_idx, cand_size)
            q = self.q_list[q_idx]
            q_word_seq = self.wd_emb_util.surface_split_tokenize(q)
            gather_linkings = self.q_link_dict[q_idx]
            '''
                In the current SimpQ experiment, there's only one focus entity (ground truth)
                so we just perform a very simple replacement
            '''
            focus_link = gather_linkings[0]
            q_word_seq_with_ph = self.add_placeholder_to_q(q_word_seq=q_word_seq,
                                                           replace_links=[focus_link])
            q_words = self.get_word_idx_list_from_string(word_seq=q_word_seq_with_ph, w_dict=w_dict)
            # now we've got the index list of the question (with placeholder)
            if len(q_words) > self.q_max_len:
                q_words = q_words[:self.q_max_len]
            q_words_dict[q_idx] = q_words

            for sc_idx, sc in enumerate(self.q_cand_dict[q_idx]):
                assert isinstance(sc, Schema)
                assert len(sc.path_list) == 1       # only one path is allowed in SimpQ
                assert len(sc.path_list[0]) == 1    # only 1-hop predicate is allowed in SimpQ
                sc.use_idx = data_idx               # set the global index of the schema
                sc.path_words_list = []

                pred = sc.path_list[0][0]       # got the only one predicate
                pred_idx = p_dict[pred]         # the predicate must be in the p_dict
                pred_name = self.fb_helper.get_item_name(pred)
                sc.path_words_list = [[pred_name]]      # just one path, and one predicate

                preds_len[data_idx] = 1
                preds[data_idx, 0] = pred_idx
                pred_word_seq = self.wd_emb_util.surface_split_naive(surface=pred_name)
                item_indices = self.get_word_idx_list_from_string(word_seq=pred_word_seq, w_dict=w_dict)
                item_len = min(len(item_indices), pword_max_len)
                pwords_len[data_idx] = item_len
                pwords[data_idx, :item_len] = item_indices[:item_len]
                data_idx += 1

        assert data_idx == cand_size
        LogInfo.end_track('np_data build complete.')

        # Final Step: create initial embedding matrix for word/entity/predicate
        LogInfo.begin_track('now creating actual initial embedding matrix ...')
        w_init_emb, e_init_emb, p_init_emb = self.create_wep_init_emb(w_dict, e_dict, p_dict)
        LogInfo.end_track()

        return q_words_dict, np_data, w_dict, e_dict, p_dict, w_init_emb, e_init_emb, p_init_emb

    def build_schema_np_data_complex(self):
        """
        Goal: given all the candidate schemas, build all the necessary numpy input
        ** Used in WebQ and SimpQ **
        ** Used after 12/06/2017 **
        ** Target models: compq_optm_model, compq_eval_model **
        :return: A list of following information
        1. q_words_dict, the word-index tensor of each question <q_idx, word_index_sequence>
        2. np_data, all the numpy data as the input of the tensor
        3. w/e/p_dict, the actual <word/mid, index> dictionary (ignoring unused items)
        4. w/e/p_init_emb, the corresponding initial embedding parameter
        """
        q_max_len = self.q_max_len
        sc_max_len = self.sc_max_len
        path_max_len = self.path_max_len
        pword_max_len = self.path_max_len * self.item_max_len
        # restrict the max length of path and path words

        # Step 1: Allocate memory for saving numpy data of all the candidates
        cand_size = sum([len(v) for v in self.q_cand_dict.values()])
        LogInfo.begin_track('Building np data for %d candidates:', cand_size)
        e_dict, p_dict = self.load_necessary_entity_predicate_dict()        # with padding item --> index = 0
        for e, e_idx in e_dict.items():
            if e != '' and e not in p_dict:
                p_dict[e] = len(p_dict)
        '''
            Major change: the information in e_dict is moved to p_dict.
            Therefore, p_dict contains all <mid, index> information and we do not need e_dict any more.
        '''

        w_dict = {'': 0}
        # <word, idx>: used for constructing the actual initial embedding matrix
        # with padding word --> index = 0
        q_words_dict = {}
        np_data = []
        for shape in [(cand_size, q_max_len),                   # qwords
                      (cand_size, ),                            # qwords_len
                      (cand_size, ),                            # sc_len
                      (cand_size, sc_max_len, path_max_len),    # preds
                      (cand_size, sc_max_len),                  # preds_len
                      (cand_size, sc_max_len, pword_max_len),   # pwords
                      (cand_size, sc_max_len)]:                 # pwords_len
            LogInfo.logs('Adding tensor with shape %s ... ', shape)
            np_data.append(np.zeros(shape=shape, dtype='int32'))
        [qwords, qwords_len, sc_len, preds, preds_len, pwords, pwords_len] = np_data
        self.array_num = len(np_data)
        '''Different qwords (with placeholder) for different schemas'''

        # Step 2: Scan each candidate, get ready to fill in with their numpy data
        data_idx = 0
        q_list = sorted(self.q_cand_dict.keys())
        for scan_idx, q_idx in enumerate(q_list):
            if scan_idx % 500 == 0:
                LogInfo.logs('%d question scanned, filled with %d / %d schemas.',
                             scan_idx, data_idx, cand_size)
            q = self.q_list[q_idx]
            qword_seq = self.wd_emb_util.surface_split_tokenize(q)

            for sc_idx, sc in enumerate(self.q_cand_dict[q_idx]):
                assert isinstance(sc, CompqSchema)  # not Schema, but CompqSchema
                sc.use_idx = data_idx               # set the global index of the schema
                sc.path_words_list = []

                # First: adding placeholders to the question, save into qwords/qwords_len
                qword_seq_with_ph = self.add_placeholder_to_q(q_word_seq=qword_seq,
                                                              replace_links=sc.replace_linkings)
                qword_indices = self.get_word_idx_list_from_string(word_seq=qword_seq_with_ph,
                                                                   w_dict=w_dict)
                use_qword_len = min(len(qword_indices), q_max_len)
                qwords_len[data_idx] = use_qword_len
                qwords[data_idx, :use_qword_len] = qword_indices[:use_qword_len]

                # Then: deal with each mid sequence
                sc_len[data_idx] = len(sc.path_list)
                for path_idx, mid_seq in enumerate(sc.path_list):   # enumerate each mid sequence
                    local_words = []
                    preds_len[data_idx, path_idx] = len(mid_seq)
                    for pos, mid in enumerate(mid_seq):
                        mid_idx = p_dict[mid]   # note: all E/T/P information are combined and stored in the p_dict
                        mid_name = self.fb_helper.get_item_name(mid)
                        local_words.append(mid_name)
                        preds[data_idx, path_idx, pos] = mid_idx
                    sc.path_words_list.append(local_words)
                    pword_indices = []      # saving all the indices of words in one mid sequence
                    for mid_name in local_words:
                        mid_word_seq = self.wd_emb_util.surface_split_naive(surface=mid_name)
                        pword_indices += self.get_word_idx_list_from_string(word_seq=mid_word_seq, w_dict=w_dict)
                    use_pword_len = min(len(pword_indices), pword_max_len)
                    pwords_len[data_idx, path_idx] = use_pword_len
                    pwords[data_idx, path_idx, :use_pword_len] = pword_indices[:use_pword_len]

                data_idx += 1

        assert data_idx == cand_size
        LogInfo.end_track('np_data build complete.')

        # Final Step: create initial embedding matrix for word/entity/predicate
        LogInfo.begin_track('now creating actual initial embedding matrix ...')
        w_init_emb, e_init_emb, p_init_emb = self.create_wep_init_emb(w_dict, e_dict, p_dict)
        LogInfo.end_track()

        return q_words_dict, np_data, w_dict, e_dict, p_dict, w_init_emb, e_init_emb, p_init_emb

    # ================ Utility Functions ================ #

    def load_necessary_entity_predicate_dict(self):
        """
        Scan FB E/T/P names, just keeping <mid, index> pairs which occur in the candidate pool
        :return: <mid, index> dictionary for both entities (including types) and predicates
        """
        e_set = set([])
        t_set = set([])
        p_set = set([])  # the sets maintaining all the entries observed in the current candidates
        for cand_list in self.q_cand_dict.values():
            for cand in cand_list:
                cand.update_item_set(e_set=e_set, t_set=t_set, p_set=p_set)
        LogInfo.logs('%d E + %d T + %d P collected.', len(e_set), len(t_set), len(p_set))
        self.fb_helper.load_names(e_set=e_set, t_set=t_set, p_set=p_set)

        e_dict = {'': 0}        # give index 0 to represent empty entity(for padding)
        for item_set in (e_set, t_set):
            for item in item_set:
                e_dict[item] = len(e_dict)
        # e_dict = {e: e_idx for e_idx, e in enumerate(e_set)}
        # e_dict.update({t: t_idx + len(e_dict) for t_idx, t in enumerate(t_set)})

        p_dict = {p: p_idx + 1 for p_idx, p in enumerate(p_set)}
        p_dict[''] = 0      # also give index 0 to represent empty predicate (for padding)
        # p_dict = {p: p_idx for p_idx, p in enumerate(p_set)}

        return e_dict, p_dict

    @staticmethod
    def add_placeholder_to_q(q_word_seq, replace_links):
        """
        Given the word sequence and the entity linking detail,
        replace the mention words by <e>
        :param q_word_seq: The word sequence of the question (tokenized)
        :param replace_links: A list of links to be replaced.
                              Each: namedtuple (category, detail, comparison, display)
        :return: The word sequence after replacement
        """
        target_word_seq = list(q_word_seq)
        for link in replace_links:
            link_cate = link.category
            ph = {'Entity': '<e>', 'Type': '<t>', 'Time': '<tm>'}[link_cate]    # get the corresponding placeholder
            link_detail = link.detail
            if len(link_detail.tokens) == 0:        # fail to find the linking position
                continue
            st = link_detail.tokens[0].index
            ed = link_detail.tokens[-1].index
            if st == -1 or ed == -1:        # error mention position
                continue
            target_word_seq[st] = ph
            for idx in range(st+1, ed+1):
                target_word_seq[idx] = ''
        ret_word_list = filter(lambda x: x != '', target_word_seq)  # remove all empty words
        return ret_word_list

    @staticmethod
    def get_word_idx_list_from_string(word_seq, w_dict):
        """
        Given the word sequence (may contains placeholder <e>),
        we gather into the list and update the dictionary
        :param word_seq: the word sequence
        :param w_dict: the word dictionary to be updated
        :return: a list of word indices
        """
        wd_idx_list = []
        for wd in word_seq:
            # if wd not in wd_emb_util.wd_idx_dict:       # ignore rare words
            #     continue
            wd_idx = w_dict.setdefault(wd, len(w_dict))
            wd_idx_list.append(wd_idx)
        return wd_idx_list

    def create_wep_init_emb(self, w_dict, e_dict, p_dict):
        """
        Provided with <word/mid, index> dictionary in use, we check the global embedding matrix,
        and generate the small w/e/p_init_emb, which is used in the learning model
        :return: word/entity/predicate_init_emb
                 three numpy matrix with (actual_size, emb_dim) shape
        """
        self.wd_emb_util.load_words()
        self.wd_emb_util.load_embeddings()
        self.kb_emb_util.load_entity_ids()
        self.kb_emb_util.load_entity_embeddings()
        self.kb_emb_util.load_predicate_ids()
        self.kb_emb_util.load_predicate_embeddings()
        w_init_emb = self.init_emb(name='word', actual_dict=w_dict,
                                   full_dict=self.wd_emb_util.wd_idx_dict,
                                   full_mat=self.wd_emb_util.emb_matrix,
                                   dim_emb=self.wd_emb_util.dim_wd_emb)
        e_init_emb = self.init_emb(name='entity', actual_dict=e_dict,
                                   full_dict=self.kb_emb_util.e_idx_dict,
                                   full_mat=self.kb_emb_util.e_emb_matrix,
                                   dim_emb=self.kb_emb_util.dim_kb_emb)
        p_init_emb = self.init_emb(name='predicate', actual_dict=p_dict,
                                   full_dict=self.kb_emb_util.p_idx_dict,
                                   full_mat=self.kb_emb_util.p_emb_matrix,
                                   dim_emb=self.kb_emb_util.dim_kb_emb)
        return w_init_emb, e_init_emb, p_init_emb

    @staticmethod
    def init_emb(name, actual_dict, full_dict, full_mat, dim_emb):
        """
        Given the actual entries and the full embedding info, construct the actual initial embedding matrix
        :param name: word/entity/predicate
        :param actual_dict: the dict storing actual entries <item, idx>
        :param full_dict: the full dict of entries <item, idx>
        :param full_mat: the full embedding matrix in numpy format
        :param dim_emb: embedding dimension
        :return: the actual initial embedding matrix in numpy format
        """
        if full_mat is not None:
            assert dim_emb == full_mat.shape[1]
        actual_size = len(actual_dict)
        ret_emb_matrix = np.random.uniform(
            low=-0.1, high=0.1, size=(actual_size, dim_emb)).astype('float32')
        # [-0.1, 0.1] as random initialize.
        for item, target_row_idx in actual_dict.items():
            if item in full_dict and full_mat is not None:
                # full_mat is None: we don't use TransE as initial embedding
                original_row_idx = full_dict[item]
                ret_emb_matrix[target_row_idx] = full_mat[original_row_idx]
        LogInfo.logs('%s: build %s actual init embedding matrix from full matrix with shape %s.',
                     name, ret_emb_matrix.shape,
                     full_mat.shape if full_mat is not None else '[None]')
        return ret_emb_matrix

    def get_schema_tensor_inputs(self, schema):
        use_idx = schema.use_idx
        tensor_inputs = [np_data[use_idx] for np_data in self.np_data_list]
        return tensor_inputs

    # ================ Save & Load Different Components ================ #

    def save_size(self):
        with open(self.size_fp, 'w') as bw:
            bw.write('words\t%d\n' % self.w_size)
            bw.write('entities\t%d\n' % self.e_size)
            bw.write('predicates\t%d\n' % self.p_size)
            bw.write('array_num\t%d\n' % self.array_num)
        LogInfo.logs('W/E/P/ArrNum size saved.')

    def load_size(self):
        if not os.path.isfile(self.dump_fp):
            self.prepare_all_data()
            return
        with open(self.size_fp, 'r') as br:
            self.w_size = int(br.readline().strip().split('\t')[1])
            self.e_size = int(br.readline().strip().split('\t')[1])
            self.p_size = int(br.readline().strip().split('\t')[1])
            self.array_num = int(br.readline().strip().split('\t')[1])
        LogInfo.logs('W size = %d, E size = %d, P size = %d, ArrNum size = %d.',
                     self.w_size, self.e_size, self.p_size, self.array_num)

    def save_dicts(self):
        LogInfo.begin_track('Saving actual w/e/p dict into [%s] ...', self.dict_fp)
        with open(self.dict_fp, 'w') as bw:
            cPickle.dump(self.w_dict, bw)
            LogInfo.logs('%d w_dict saved.', len(self.w_dict))
            cPickle.dump(self.e_dict, bw)
            LogInfo.logs('%d e_dict saved.', len(self.e_dict))
            cPickle.dump(self.p_dict, bw)
            LogInfo.logs('%d p_dict saved.', len(self.p_dict))
        LogInfo.end_track()

    def load_dicts(self):
        if self.w_dict is not None and \
                self.e_dict is not None and \
                self.p_dict is not None:
            return
        if not os.path.isfile(self.dict_fp):
            self.prepare_all_data()
            return
        LogInfo.begin_track('Loading w/e/p dictionaries from [%s] ...', self.dict_fp)
        with open(self.dict_fp, 'rb') as br:
            self.w_dict = cPickle.load(br)
            LogInfo.logs('w_dict: %d loaded.', len(self.w_dict))
            self.e_dict = cPickle.load(br)
            LogInfo.logs('e_dict: %d loaded.', len(self.e_dict))
            self.p_dict = cPickle.load(br)
            LogInfo.logs('p_dict: %d loaded.', len(self.p_dict))
        LogInfo.end_track()

    def save_init_emb(self):
        LogInfo.begin_track('Saving initial embedding matrix into [%s] ...', self.init_mat_fp)
        with open(self.init_mat_fp, 'wb') as bw:
            np.save(bw, self.w_init_emb)
            LogInfo.logs('word embedding: %s', self.w_init_emb.shape)
            np.save(bw, self.e_init_emb)
            LogInfo.logs('entity embedding: %s', self.e_init_emb.shape)
            np.save(bw, self.p_init_emb)
            LogInfo.logs('predicate embedding: %s', self.p_init_emb.shape)
        LogInfo.end_track()

    def load_init_emb(self):
        if self.w_init_emb is not None and \
                self.e_init_emb is not None and \
                self.p_init_emb is not None:
            return
        if not os.path.isfile(self.dump_fp):
            self.prepare_all_data()
            return
        LogInfo.begin_track('Loading initial embedding matrix from [%s] ...', self.init_mat_fp)
        with open(self.init_mat_fp, 'rb') as br:
            self.w_init_emb = np.load(br)
            LogInfo.logs('word embedding: %s', self.w_init_emb.shape)
            self.e_init_emb = np.load(br)
            LogInfo.logs('entity embedding: %s', self.e_init_emb.shape)     # including entities, numeric/time/ordinal
            self.p_init_emb = np.load(br)
            LogInfo.logs('predicate embedding: %s', self.p_init_emb.shape)  # including forward and backward predicates
        LogInfo.end_track()

    def save_cands(self):
        LogInfo.begin_track('Saving candidates & np_data into [%s] ...', self.dump_fp)
        with open(self.dump_fp, 'wb') as bw:
            cPickle.dump(self.q_list, bw)
            LogInfo.logs('%d q_list saved.', len(self.q_list))
            cPickle.dump(self.q_words_dict, bw)
            LogInfo.logs('%d q_words_dict saved.', len(self.q_words_dict))
            cPickle.dump(self.q_cand_dict, bw)
            LogInfo.logs('%d q_cand_dict saved.', len(self.q_cand_dict))
            assert len(self.np_data_list) == self.array_num
            for data_idx, np_data in enumerate(self.np_data_list):
                np.save(bw, np_data)
                LogInfo.logs('np-data-%d saved: %s', data_idx, np_data.shape)
        LogInfo.end_track()

    def load_cands(self):
        if len(self.np_data_list) > 0 and \
                self.q_cand_dict is not None and \
                self.q_words_dict is not None:
            return
        if not os.path.isfile(self.dump_fp):
            self.prepare_all_data()
            return
        LogInfo.begin_track('Loading candidates & np_data from [%s] ...', self.dump_fp)
        with open(self.dump_fp, 'rb') as br:
            self.q_list = cPickle.load(br)
            LogInfo.logs('q_list loaded for %d questions.', len(self.q_list))
            self.q_words_dict = cPickle.load(br)
            LogInfo.logs('q_words_dict loaded for %d questions.', len(self.q_words_dict))
            self.q_cand_dict = cPickle.load(br)
            LogInfo.logs('q_cand_dict loaded.')

            cand_size_dist = np.array([len(v) for v in self.q_cand_dict.values()])
            LogInfo.begin_track('Show candidate size distribution:')
            for pos in (25, 50, 75, 90, 95, 99, 99.9, 100):
                LogInfo.logs('Percentile = %.1f%%: %.6f', pos, np.percentile(cand_size_dist, pos))
            LogInfo.end_track()

            for data_idx in range(self.array_num):
                np_data = np.load(br)
                self.np_data_list.append(np_data)
                LogInfo.logs('np-data-%d loaded: %s', data_idx, np_data.shape)
        LogInfo.end_track()


def main(args):
    from .u import load_webq
    from ..util.word_emb import WordEmbeddingUtil
    from ..util.kb_emb import KBEmbeddingUtil
    from ..util.fb_helper import FreebaseHelper

    kq_name = args[1]
    LogInfo.begin_track('Dataset comparison between XY and KQ ... ')
    qa_list = load_webq()
    webq_list = [qa['utterance'] for qa in qa_list]

    config_dict = {
        'q_max_len': 15,
        'sc_max_len': 3,
        'path_max_len': 2,
        'item_max_len': 5,
        'webq_list': webq_list,
        'wd_emb_util': WordEmbeddingUtil(wd_emb='glove', dim_wd_emb=300, parser_ip='202.120.38.146', parser_port=9601),
        'kb_emb_util': KBEmbeddingUtil(kb_emb_dir='data/kb_emb/FB2M', dim_kb_emb=200),
        'fb_helper': FreebaseHelper('data/fb_metadata'),
        'verbose': 1
    }

    LogInfo.begin_track('Loading XY-Dataset ... ')
    xy_config_dict = dict(config_dict)
    xy_config_dict['data_dir'] = 'data/compQA/xy_q_schema'
    xy_config_dict['file_list_name'] = 'cmp_list'
    xy_config_dict['protocol_name'] = 'XY'
    xy_dataset = QScDataset(**xy_config_dict)
    xy_dataset.load_cands()
    LogInfo.end_track()

    LogInfo.begin_track('Loading KQ-Dataset ... ')
    kq_config_dict = dict(config_dict)
    kq_config_dict['data_dir'] = 'runnings/candgen/' + kq_name
    kq_config_dict['file_list_name'] = 'cmp_list'
    kq_config_dict['protocol_name'] = 'KQ'
    kq_dataset = QScDataset(**kq_config_dict)
    kq_dataset.load_cands()
    LogInfo.end_track()

    LogInfo.begin_track('Now compare:')
    for q_idx in kq_dataset.q_cand_dict.keys():
        if q_idx not in xy_dataset.q_cand_dict:
            continue
        LogInfo.begin_track('Q-%d [%s]:', q_idx, webq_list[q_idx].encode('utf-8'))
        for name, ds in zip(('KQ', 'XY'), (kq_dataset, xy_dataset)):
            cand_list = ds.q_cand_dict[q_idx]
            max_f1 = cand_list[0].f1 if len(cand_list) > 0 else -1.0
            max_num = len(filter(lambda sc: sc.f1 == max_f1, cand_list))
            LogInfo.logs('%s: candidates = %3d, top f1 = %.6f, top numbers = %3d.',
                         name, len(cand_list), max_f1, max_num)
            if max_f1 > 0.:         # show the top f1 schema
                for cand in cand_list:
                    if cand.f1 < max_f1:
                        break
                    LogInfo.logs(cand.disp())
        LogInfo.end_track()
    LogInfo.end_track()

    LogInfo.end_track()


if __name__ == '__main__':
    import sys
    main(args=sys.argv)
